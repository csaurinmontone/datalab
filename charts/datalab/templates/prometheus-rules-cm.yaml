apiVersion: v1
kind: ConfigMap
metadata:
  name: "prometheus-alerts"
  labels:
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    helm.sh/chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
data:
    alerting_rules.yml: |
      groups:
        - name: Quotas
          rules:
            - alert: CpuRequestQuotaExceeded
              annotations:
                description: '{{ "{{" }} $labels.exported_namespace }} has been over requesting cpu in the Onyxia services'
                summary: '{{ "{{" }} $labels.exported_namespace }} cpu base request exceeded 0.5 cores'
              expr: sum by (exported_namespace) (kube_pod_container_resource_requests{exported_namespace=~"user-.*", resource="cpu"}) > {{ .Values.alertThresholds.CpuRequestQuota }}
              for: 10m
              labels:
                severity: warning
            - alert: MemRequestQuotaExceeded
              annotations:
                description: '{{ "{{" }} $labels.exported_namespace }} has been over requesting memory in the Onyxia services'
                summary: '{{ "{{" }} $labels.exported_namespace }} memory base request exceeded 4 GB'
              expr: sum by (exported_namespace) (kube_pod_container_resource_requests{exported_namespace=~"user-.*", resource="memory"}) / 1073741824 > {{ .Values.alertThresholds.MemRequestQuota }}
              for: 10m
              labels:
                severity: warning
            - alert: CpuLimitsQuotaExceeded
              annotations:
                description: '{{ "{{" }} $labels.exported_namespace }} has been over requesting cpu in the Onyxia services'
                summary: '{{ "{{" }} $labels.exported_namespace }} cpu self-set limits exceeded 30 cores'
              expr: sum by (exported_namespace) (kube_pod_container_resource_limits{exported_namespace=~"user-.*", resource="cpu"}) > {{ .Values.alertThresholds.CpuLimitsQuota }}
              for: 10m
              labels:
                severity: warning
            - alert: MemLimitsQuotaExceeded
              annotations:
                description: '{{ "{{" }} $labels.exported_namespace }} has been over requesting memory in the Onyxia services'
                summary: '{{ "{{" }} $labels.exported_namespace }} memory self-set limits exceeded 64 GB'
              expr: sum by (exported_namespace) (kube_pod_container_resource_limits{exported_namespace=~"user-.*", resource="memory"}) / 1073741824 > {{ .Values.alertThresholds.MemLimitsQuota }}
              for: 10m
              labels:
                severity: warning
        - name: Usage
          rules:
            - alert: UserCpuHogging
              annotations:
                description: '{{ "{{" }} $labels.namespace }} instances are hogging more than 80% CPU in node {{ "{{" }} $labels.kubernetes_io_hostname}} in the past 30m'
                summary: '{{ "{{" }} $labels.namespace }} instances are hogging more than 80% CPU in node {{ "{{" }} $labels.kubernetes_io_hostname}}'
              expr: sum by (kubernetes_io_hostname, namespace) (rate (container_cpu_usage_seconds_total{namespace=~"user-.*", image!="", container!="POD"}[30m])) / ignoring(namespace)  group_left sum by (kubernetes_io_hostname)(machine_cpu_cores) > 0.8
              for: 0m
              labels:
                severity: critical
            - alert: UserMemoryHogging
              annotations:
                description: '{{ "{{" }} $labels.namespace }} instances are hogging more than 80% of memory in node {{ "{{" }} $labels.kubernetes_io_hostname}} in the past 30m'
                summary: '{{ "{{" }} $labels.namespace }} instances are hogging more than 80% of memory in node {{ "{{" }} $labels.kubernetes_io_hostname}}'
              expr: sum by (kubernetes_io_hostname, namespace) (rate (container_memory_usage_bytes{namespace=~"user-.*", container!="POD", image!=""}[30m])) / ignoring(namespace)  group_left sum by (kubernetes_io_hostname)(machine_memory_bytes) > 0.8
              for: 0m
              labels:
                severity: critical
        - name: Inactivity
          rules:
            - alert: UserInactive
              annotations:
                description: 'User {{ "{{" }} $labels.user }} (username) might be inactive'
                summary: 'This warning was triggered due to the user ( {{ "{{" }} $labels.user }} ) not interacting with any of the authetication services (Keycloak) in the past {{ .Values.alertThresholds.inactivityPeriod }}'
              expr: rate(keycloak_registered_events_by_user{user=~".*"}[{{ .Values.alertThresholds.inactivityPeriod }}]) == 0
              for: 1d
              labels:
                severity: warning
            - alert: InstanceInactive
              annotations:
                description: 'Instance with ingress {{ "{{" }} $labels.ingress }} (ingress name matches pod name) might be inactive'
                summary: 'This warning was triggered due to the intance {{ "{{" }} $labels.ingress }} not being accessed in the past {{ .Values.alertThresholds.inactivityPeriod }}. The pod CPU and network usage should be checked as well before contacting the instance creator or deleting the pod.'
              expr: (((sum (kube_ingress_created{exported_namespace=~"user-.*"} <= (time() - 300)) by (ingress) - sum (kube_ingress_created{exported_namespace=~"user-.*"} <= (time() - 300)) by (ingress))) + on(ingress) (sum(rate(nginx_ingress_controller_requests{exported_namespace=~"user-.*"}[{{ .Values.alertThresholds.inactivityPeriod }}])) by (ingress)) or on(ingress) ((sum (kube_ingress_created{exported_namespace=~"user-.*"} <= (time() - 300)) by (ingress) - sum (kube_ingress_created{exported_namespace=~"user-.*"} <= (time() - 300)) by (ingress)))) == 0
              for: 1d
              labels:
                severity: warning